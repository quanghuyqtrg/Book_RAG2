# streamlit_app.py
import base64
import json
from pathlib import Path
from typing import Dict, Tuple, Optional, List

import streamlit as st
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

# Local modules
from app import ingest as ingest_mod
from app import assistant as ra
from app.query import run_query

# ---------------------- Init & Config ----------------------
PROJECT_ROOT = Path(__file__).resolve().parents[0]
# Use the same DATA_DIR convention as ingest.py so both point to one folder
DATA_DIR: Path = (PROJECT_ROOT / "data" / "books")
DATA_DIR.mkdir(parents=True, exist_ok=True)

load_dotenv()
load_dotenv()
PGUSER = "postgres"
PGPASSWORD = "husky"
PGDATABASE = "postgres"
PGPORT = 55432
st.set_page_config(page_title="AI Reading Assistant", page_icon="ğŸ“–", layout="wide")

# DB engine (táº­n dá»¥ng config tá»« ingest_mod)
ENGINE = create_engine(
    f"postgresql+psycopg2://{ingest_mod.PGUSER}:{ingest_mod.PGPASSWORD}"
    f"@localhost:{ingest_mod.PGPORT}/{ingest_mod.PGDATABASE}"
)

# Ensure schema exists on first load (idempotent)
try:
    ingest_mod.ensure_schema()
except Exception:
    pass

# ---------------------- DB helpers ----------------------

def list_books() -> List[Dict]:
    sql = """
    SELECT b.id::text AS book_id, b.title,
           COALESCE(MIN(p.page_no), 1) AS min_page,
           COALESCE(MAX(p.page_no), 0) AS max_page,
           COUNT(p.page_id) AS total_pages
    FROM books b
    LEFT JOIN pages p ON p.book_id = b.id
    GROUP BY b.id, b.title
    ORDER BY b.title ASC;
    """
    with ENGINE.begin() as conn:
        rows = conn.execute(text(sql)).mappings().all()
    return list(rows)


def get_reading_output(book_id: str, type_: str, page_range: Optional[Tuple[int, int]] = None) -> Optional[Dict]:
    if page_range:
        sql = """
        SELECT content
        FROM reading_outputs
        WHERE book_id = :b AND type = :t AND page_span = int4range(:s, :e, '[]')
        ORDER BY created_at DESC
        LIMIT 1;
        """
        params = {"b": book_id, "t": type_, "s": page_range[0], "e": page_range[1]}
    else:
        sql = """
        SELECT content
        FROM reading_outputs
        WHERE book_id = :b AND type = :t AND page_span IS NULL
        ORDER BY created_at DESC
        LIMIT 1;
        """
        params = {"b": book_id, "t": type_}
    with ENGINE.begin() as conn:
        row = conn.execute(text(sql), params).fetchone()
    return row[0] if row else None


# ---------------------- UI state ----------------------

def _ensure_session_defaults():
    ss = st.session_state
    ss.setdefault("messages", [])
    ss.setdefault("selected_book_id", None)
    ss.setdefault("selected_range", {"start": None, "end": None})
    ss.setdefault("outline_cache", {})  # {book_id: outline_json}


_ensure_session_defaults()


# ---------------------- Utilities ----------------------

def _pretty_json(obj: Dict) -> str:
    try:
        import json as _json
        return _json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return str(obj)


def _find_pdf_for_book(title: str) -> Optional[Path]:
    # 1) khá»›p chÃ­nh xÃ¡c "<title>.pdf"
    direct = DATA_DIR / f"{title}.pdf"
    if direct.exists():
        return direct
    # 2) tÃ¬m gáº§n Ä‘Ãºng theo stem
    candidates = list(DATA_DIR.glob("*.pdf"))
    # Æ°u tiÃªn tÃªn chá»©a nguyÃªn cá»¥m title (khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng)
    title_l = title.lower()
    scored = sorted(
        candidates,
        key=lambda p: (0 if title_l in p.stem.lower() else 1, abs(len(p.stem) - len(title))),
    )
    return scored[0] if scored else None


def _embed_pdf(pdf_path: Path, height: int = 900):
    data = pdf_path.read_bytes()
    b64 = base64.b64encode(data).decode("utf-8")
    html = f"""
    <iframe src="data:application/pdf;base64,{b64}#toolbar=1&navpanes=1&scrollbar=1"
            style="width:100%;height:{height}px;border:none;"></iframe>
    """
    st.components.v1.html(html, height=height, scrolling=False)


def _chip(label: str, key: str) -> bool:
    return st.button(label, key=key, use_container_width=True)


def _ask_and_render(question: str):
    # LÆ°u tin nháº¯n ngÆ°á»i dÃ¹ng
    st.session_state.messages.append(("user", question))
    with st.chat_message("user"):
        st.markdown(question)

    # Tráº£ lá»i: chá»‰ nhá»¯ng gÃ¬ query.py sinh ra (khÃ´ng filter theo book_id)
    with st.chat_message("assistant"):
        with st.spinner("Äang suy nghÄ©â€¦"):
            try:
                # LuÃ´n tÃ¬m trÃªn toÃ n bá»™ thÆ° viá»‡n (bá» book_id)
                res = run_query(question, k=20, book_id=None)

                # run_query Ä‘Æ°á»£c ká»³ vá»ng tráº£ vá» dict; phÃ²ng há» náº¿u khÃ¡c
                if not isinstance(res, dict):
                    res = {"answer": str(res)}

                # Hiá»ƒn thá»‹ nguyÃªn vÄƒn answer do query.py tráº£ vá»
                answer_text = res.get("answer") or ""
                st.markdown(answer_text)

                # Hiá»ƒn thá»‹ toÃ n bá»™ "raw output" Ä‘Ãºng nhÆ° query.py tráº£ vá»
                with st.expander("Chi tiáº¿t (raw output tá»« query.py)"):
                    st.code(json.dumps(res, ensure_ascii=False, indent=2), language="json")

                # LÆ°u vÃ o session
                st.session_state.messages.append(("assistant", answer_text))

            except Exception as e:
                st.error("âŒ Lá»—i khi xá»­ lÃ½ yÃªu cáº§u.")
                st.exception(e)
                st.session_state.messages.append(("assistant", f"âŒ {e}"))


# match PDF file for title

def find_pdf_for_title(title: str) -> Path | None:
    # exact match
    p = DATA_DIR / f"{title}.pdf"
    if p.exists():
        return p
    # fuzzy contains
    cands = sorted(DATA_DIR.glob("*.pdf"), key=lambda x: (title.lower() not in x.stem.lower(), abs(len(x.stem)-len(title))))
    return cands[0] if cands else None


# --- Markdown render helpers ---

def _md_summary(content: Dict) -> str:
    title = content.get("title", "Summary")
    abstract = content.get("abstract", "").strip()
    bullets = content.get("bullets", [])[:10]
    takeaways = content.get("takeaways", [])[:6]
    bl = "\n".join(f"- {b}" for b in bullets) or "- (khÃ´ng cÃ³)"
    tk = "\n".join(f"â€¢ {t}" for t in takeaways) or "â€¢ (khÃ´ng cÃ³)"
    return f"""**{title}**

{abstract}

**Key points**
{bl}

**Takeaways**
{tk}
"""


def _md_keypoints(content: Dict) -> str:
    kps = content.get("key_points", [])[:10]
    cons = content.get("conclusions", [])[:3]
    kp_md = "\n".join(f"- {k}" for k in kps) or "- (khÃ´ng cÃ³)"
    c_md = "\n".join(f"- {c}" for c in cons) or "- (khÃ´ng cÃ³)"
    return f"""**Key points**
{kp_md}

**Conclusions**
{c_md}
"""


def _md_quotes(content: Dict) -> str:
    qts = content.get("quotes", [])[:6]
    if not qts:
        return "> (khÃ´ng trÃ­ch dáº«n nÃ o)"
    lines = []
    for q in qts:
        qtext = q.get("quote", "").strip()
        p = q.get("approx_page", "?")
        th = q.get("theme", "")
        lines.append(f"> {qtext}\n\nâ€” p.{p} Â· {th}")
    return "\n\n".join(lines)


# ---------------------- Layout ----------------------
left, right = st.columns([7, 3], gap="large")

# ===== RIGHT: AI Reading Assistant (chips + chat) =====
with right:
    st.markdown("### ğŸ¤– AI Reading Assistant")
    st.caption("Há»i Ä‘Ã¡p vá» ná»™i dung sÃ¡ch")

    st.markdown("**ğŸ’¡ CÃ¢u há»i gá»£i Ã½ â€“ Click Ä‘á»ƒ há»i ngay:**")
    c1, c2 = st.columns(2)
    with c1:
        if _chip("TÃ³m táº¯t chÆ°Æ¡ng 1", "sg_s1"):
            _ask_and_render("TÃ³m táº¯t chÆ°Æ¡ng 1 cá»§a cuá»‘n sÃ¡ch nÃ y (nÃªu cÃ¡c Ä‘iá»ƒm chÃ­nh vÃ  sá»‘ trang liÃªn quan).")
        if _chip("Äiá»ƒm chÃ­nh cuá»‘n sÃ¡ch", "sg_p1"):
            _ask_and_render("Liá»‡t kÃª cÃ¡c Ä‘iá»ƒm chÃ­nh (key points) cá»§a cuá»‘n sÃ¡ch, kÃ¨m sá»‘ trang tham chiáº¿u.")
        if _chip("CÃ¡c vÃ­ dá»¥ thá»±c táº¿", "sg_ex"):
            _ask_and_render("TÃ¬m cÃ¡c vÃ­ dá»¥/thá»±c nghiá»‡m thá»±c táº¿ tiÃªu biá»ƒu trong sÃ¡ch, tÃ³m lÆ°á»£c ngáº¯n gá»n.")
    with c2:
        if _chip("Gá»£i Ã½ pháº§n nÃªn Ä‘á»c", "sg_rec"):
            _ask_and_render("Gá»£i Ã½ nhá»¯ng pháº§n nÃªn Ä‘á»c trÆ°á»›c (vÃ¬ dá»… tiáº¿p cáº­n/quan trá»ng), kÃ¨m trang.")
        if _chip("TÃ¬m phÃ¡t biá»ƒu quan trá»ng", "sg_quote"):
            _ask_and_render("TÃ¬m cÃ¡c phÃ¡t biá»ƒu/Ä‘á»‹nh nghÄ©a quan trá»ng (trÃ­ch dáº«n nguyÃªn vÄƒn) vÃ  trang.")
        if _chip("Káº¿t luáº­n chÃ­nh", "sg_conc"):
            _ask_and_render("TÃ³m táº¯t cÃ¡c káº¿t luáº­n chÃ­nh cá»§a sÃ¡ch, kÃ¨m trang náº¿u cÃ³.")

    st.markdown("---")

    # Lá»‹ch sá»­ + input chat (chá»‰ 1 khung chat)
    for role, content in st.session_state.messages:
        with st.chat_message(role):
            st.markdown(content)

    user_q = st.chat_input("Há»i vá» ná»™i dung sÃ¡châ€¦")
    if user_q:
        _ask_and_render(user_q)

# ===== LEFT: Viewer + ingest panel =====
with left:
    st.markdown("### ğŸ“š ThÆ° viá»‡n & TrÃ¬nh Ä‘á»c")

    # --- Upload & Ingest panel ---
    with st.expander("ğŸ“¥ ThÃªm PDF vÃ o thÆ° viá»‡n", expanded=False):
        files = st.file_uploader("Chá»n PDF", type=["pdf"], accept_multiple_files=True)
        force = st.checkbox("Force re-index (ghi Ä‘Ã¨ náº¿u Ä‘Ã£ cÃ³)", value=False,
                            help="Bá» qua kiá»ƒm tra Ä‘Ã£ index. Chá»‰ dÃ¹ng khi báº¡n cháº¯c muá»‘n lÃ m má»›i vectors.")

        if files and st.button("ğŸš€ Náº¡p & index", type="primary"):
            saved_paths: List[Path] = []
            for f in files:
                dest = DATA_DIR / f.name
                dest.write_bytes(f.getbuffer())
                saved_paths.append(dest)

            with st.spinner("Äang ingest tá»«ng file vá»«a uploadâ€¦"):
                for p in saved_paths:
                    try:
                        res = ingest_mod.ingest_pdf(p, skip_if_exists=True, force=force)
                        if res.get("skipped"):
                            st.info(f"â­ï¸ Bá» qua (Ä‘Ã£ cÃ³): {res.get('file')} â€” {res.get('points')}")
                        else:
                            pts = res.get("points", {})
                            st.success(
                                f"âœ… ÄÃ£ náº¡p: {res.get('file')} â€” pages={res.get('pages')} chunks={pts.get('chunks')} seeds={pts.get('seeds')}")
                    except Exception as e:
                        st.error(f"âŒ Lá»—i ingest {p.name}: {e}")
            st.rerun()

    # 1) Láº¥y danh sÃ¡ch sÃ¡ch
    books = list_books()
    if not books:
        st.info("ChÆ°a cÃ³ sÃ¡ch nÃ o trong thÆ° viá»‡n. HÃ£y upload PDF á»Ÿ trÃªn vÃ  báº¥m Ingest.")
        st.stop()

    titles = [f"{b['title']} (pp. {b['min_page']}â€“{b['max_page']}, {b['total_pages']} trang)" for b in books]
    sel = st.selectbox(""
                       "", options=list(range(len(books))), format_func=lambda i: titles[i], key="book_select")
    book = books[sel]
    # GÃN book_id TRÆ¯á»šC khi váº½ RIGHT
    st.session_state["selected_book_id"] = book["book_id"]

    st.markdown(f"#### **{book['title']}**")
    st.caption(f"Sá»‘ trang: {book['total_pages']} (pp. {book['min_page']}â€“{book['max_page']})")

    # ---- PDF.js viewer via streamlit-pdf-viewer ----
    pdf_path = find_pdf_for_title(book["title"]) or _find_pdf_for_book(book["title"])  # fallback to fuzzy finder
    if not pdf_path:
        st.error(f"KhÃ´ng tÃ¬m tháº¥y PDF cho â€œ{book['title']}â€ trong `{DATA_DIR}`.")
    else:
        try:
            from streamlit_pdf_viewer import pdf_viewer
            pdf_bytes = pdf_path.read_bytes()
            pdf_viewer(pdf_bytes, height=900, width=0, key=f"pdfv_{pdf_path.stem}")
            st.caption(f"Äang hiá»ƒn thá»‹: {pdf_path.name} â€¢ {pdf_path.stat().st_size/1e6:.1f} MB")
        except Exception:
            _embed_pdf(pdf_path, height=900)

st.markdown("---")
st.caption("Â© AI Reading Assistant â€” Streamlit + Qdrant + Gemini")
